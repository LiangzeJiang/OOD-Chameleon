{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import *\n",
    "from utils import load_data, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"CelebA\"  # or \"CivilComments\"\n",
    "eval_data = \"MetaShift\" # or \"MultiNLI\", \"MetaShift\", \"OfficeHome\", \"ColoredMNIST\"\n",
    "model = \"resnet\" # or \"clip\", \"bert-ft\"\n",
    "classifier = \"linear\"\n",
    "learner = \"MLP\" # mlp or linear, kNN for the ablation study purpose\n",
    "seed = 0\n",
    "input_feats = ['n', 'sc', 'ci', 'ai', 'c_intra', 'a_intra']\n",
    "\n",
    "MARGIN = 0.05\n",
    "ALGORITHMS = ['ERM', 'GroupDRO', 'oversample', 'remax-margin', 'undersample']\n",
    "OUTPUT_DIR = \"YOUR_PATH\"  # should be the same as OUTPUT_DIR in configs/DATA_PATH.sh\n",
    "TRAIN_PATH = os.path.join(OUTPUT_DIR, train_data.lower())\n",
    "EVAL_PATH = os.path.join(OUTPUT_DIR, eval_data.lower())\n",
    "IDENTIFIER = ['n', 'sc', 'ci', 'ai', 'y_task', 'a_task']  # the statistics that jointly identify a task\n",
    "METRIC = 'wga_te_err'  # the metric that we want to optimize\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the task statistics and the performance of the algorithms\n",
    "train_df = load_data(model, TRAIN_PATH, classifier, ALGORITHMS, IDENTIFIER, METRIC, MARGIN)\n",
    "eval_df = load_data(model, EVAL_PATH, classifier, ALGORITHMS, IDENTIFIER, METRIC, MARGIN)\n",
    "num_datasets = len(train_df)//len(ALGORITHMS)\n",
    "\n",
    "tr_idx = np.random.choice(num_datasets, int(num_datasets*0.8), replace=False)\n",
    "val_idx = np.array([i for i in range(num_datasets) if i not in tr_idx])\n",
    "\n",
    "# sanity check\n",
    "print(model)\n",
    "print(train_df.shape, eval_df.shape)\n",
    "val_df = train_df.iloc[np.concatenate([np.arange(len(ALGORITHMS)*i, len(ALGORITHMS)*i+len(ALGORITHMS)) for i in val_idx])]\n",
    "\n",
    "print(val_df[METRIC].mean(), val_df[val_df[\"rank\"]==1.0][METRIC].mean())\n",
    "print(eval_df[METRIC].mean(), eval_df[eval_df[\"rank\"]==1.0][METRIC].mean())\n",
    "print()\n",
    "for alg in ALGORITHMS:\n",
    "    print(alg, val_df[val_df[\"algorithm\"]==alg][METRIC].mean())\n",
    "print()\n",
    "for alg in ALGORITHMS:\n",
    "    print(alg, eval_df[eval_df[\"algorithm\"]==alg][METRIC].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlc(X_train, y_train, num_epochs=800, learner=\"MLP\", verbose=True):\n",
    "    # Hyperparameters and data\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_layer_sizes = (100,)\n",
    "    output_size = len(ALGORITHMS)\n",
    "    patience = 2000\n",
    "    tol = 1e-4\n",
    "    alpha=0.0001\n",
    "    batch_size = len(X_train)\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    y_train = torch.tensor(y_train).float()\n",
    "\n",
    "    # Create dataloader\n",
    "    tr_dataset = TensorDataset(X_train, y_train)\n",
    "    tr_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    trained_model = None\n",
    "    if learner == \"MLP\":\n",
    "        model = MLPTorch(input_size, hidden_layer_sizes, output_size)\n",
    "    elif learner == \"Linear\":\n",
    "        model = LinearTorch(input_size, output_size)\n",
    "    elif learner == \"kNN\":\n",
    "        k = 5\n",
    "        trained_model = KNN(k)\n",
    "        trained_model.fit(X_train, y_train)\n",
    "    elif learner == \"DecisionTree\":\n",
    "        from sklearn import tree\n",
    "        trained_model = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
    "        trained_model.fit(X_train, y_train)\n",
    "\n",
    "    if trained_model is None:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=alpha)\n",
    "        trained_model = train_model(model, tr_dataloader, criterion, optimizer, num_epochs, patience, tol, verbose=verbose)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "def predict_mlc(model, X):\n",
    "    if not torch.is_tensor(X):\n",
    "        X = torch.tensor(X).float()\n",
    "\n",
    "    # handle torch and sklearn models differently\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_logits = model(X)\n",
    "            y_prob = torch.sigmoid(y_logits)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            y_pred = torch.nn.functional.one_hot(y_pred, num_classes=len(ALGORITHMS)).float()\n",
    "            return y_pred.numpy(), y_prob.numpy()\n",
    "    except:\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data(\"mlc\", train_df, eval_df, tr_idx, val_idx, input_feats, ALGORITHMS, METRIC, MARGIN)\n",
    "trained_model = train_mlc(data[\"X_train\"], data[\"y_train\"], learner=\"Linear\", verbose=True, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_prob = predict_mlc(trained_model, data[\"X_val\"])\n",
    "_ = eval_acc(data[\"y_val\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "expanded_val_idx = np.concatenate([np.arange(len(ALGORITHMS)*i, len(ALGORITHMS)*i+len(ALGORITHMS)) for i in val_idx])\n",
    "_ = eval_wga_err(train_df.iloc[expanded_val_idx, :], y_pred, IDENTIFIER, ALGORITHMS, METRIC, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_prob = predict_mlc(trained_model, data[\"X_test\"])\n",
    "_ = eval_acc(data[\"y_test\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "_ = eval_wga_err(eval_df, y_pred, IDENTIFIER, ALGORITHMS, METRIC, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(X_train, y_train, num_epochs=800, verbose=True):\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    trained_model = MLPRegressor(random_state=0, max_iter=num_epochs, verbose=verbose, tol=1e-4, alpha=0.1, hidden_layer_sizes=(100,)).fit(X_train, y_train)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "def predict_regression(model, X):\n",
    "    def get_rank(x):\n",
    "        min_err = x.min()\n",
    "        return (x <= min_err + MARGIN).astype(int)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_agg = []\n",
    "    assert len(y_pred) % len(ALGORITHMS) == 0\n",
    "    for i in range(len(y_pred)//len(ALGORITHMS)):\n",
    "        curr_pred = y_pred[len(ALGORITHMS)*i:len(ALGORITHMS)*i+len(ALGORITHMS)]\n",
    "        y_pred_agg.append(get_rank(curr_pred))\n",
    "    return y_pred_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data(\"regression\", train_df, eval_df, tr_idx, val_idx, input_feats, ALGORITHMS, METRIC, MARGIN)\n",
    "trained_model = train_regression(data[\"X_train\"], data[\"y_train\"], num_epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_regression(trained_model, data[\"X_val\"])\n",
    "_ = eval_acc(data[\"y_val\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "expanded_val_idx = np.concatenate([np.arange(len(ALGORITHMS)*i, len(ALGORITHMS)*i+len(ALGORITHMS)) for i in val_idx])\n",
    "_ = eval_wga_err(train_df.iloc[expanded_val_idx, :], y_pred, IDENTIFIER, ALGORITHMS, METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_regression(trained_model, data[\"X_test\"])\n",
    "_ = eval_acc(data[\"y_test\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "_ = eval_wga_err(eval_df, y_pred, IDENTIFIER, ALGORITHMS, METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 'global_best' # or \"random\"\n",
    "data = prepare_data(\"baseline\", train_df, eval_df, tr_idx, val_idx, input_feats, ALGORITHMS, METRIC, MARGIN)\n",
    "global_rank = np.array(train_df[train_df[\"rank\"]==1.0].iloc[tr_idx]['multi_hot'].tolist()).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_baseline(mode, X):\n",
    "    y_preds = []\n",
    "    for i in range(len(X)):\n",
    "        if mode == \"random\":\n",
    "            num_winners = np.random.choice(len(ALGORITHMS), 1, replace=False)[0] + 1\n",
    "            y_p = np.random.choice(len(ALGORITHMS), num_winners, replace=False)\n",
    "        elif mode == \"global_best\":\n",
    "            num_winners = np.random.choice(len(ALGORITHMS), 1, replace=False)[0] + 1\n",
    "            y_p = np.argsort(global_rank)[::-1][:num_winners]\n",
    "        else:\n",
    "            raise ValueError(f\"unknown mode {mode}\")\n",
    "        # convert y_pred to multi-hot\n",
    "        y_p = [1 if i in y_p else 0 for i in range(len(ALGORITHMS))]\n",
    "        y_preds.append(y_p)\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_baseline(baseline, data[\"X_val\"])\n",
    "_ = eval_acc(data[\"y_val\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "expanded_val_idx = np.concatenate([np.arange(len(ALGORITHMS)*i, len(ALGORITHMS)*i+len(ALGORITHMS)) for i in val_idx])\n",
    "_ = eval_wga_err(train_df.iloc[expanded_val_idx, :], y_pred, IDENTIFIER, ALGORITHMS, METRIC)\n",
    "\n",
    "y_pred = predict_baseline(baseline, data[\"X_test\"])\n",
    "_ = eval_acc(data[\"y_test\"], y_pred, mode='soft 0-1', verbose=True)\n",
    "_ = eval_wga_err(eval_df, y_pred, IDENTIFIER, ALGORITHMS, METRIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "div_backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
